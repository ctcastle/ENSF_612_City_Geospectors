{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dee90bf-6c6d-4fba-9012-ac4c40be0818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Notebook\n",
    "This notebook will contain the data processing steps for creating the bronze level of the data lake. This includes loading in the raw data from different data sources and aggregating the results. We will begin by importing all of the necessary packages and creating variables that will be consistent throughout the entire notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "393a8461-766f-4925-b669-73d40decc2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports\n",
    "We first need to import all of the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c6743b-599d-4897-804e-147d8947246c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from datetime import date, timedelta, datetime\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3134474d-0cab-475e-b455-eaaa827a5bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "Now we need to setup the necessary classes that will be used to extract the raw data into the bronze layer of the datalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90406eae-b7c6-463f-80fa-6a415eec0d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Immutable datatypes that will be used throughout the project\n",
    "class AzureDataLake(NamedTuple):\n",
    "    \"\"\"\n",
    "    Immutable object that contains all of the necessary data needed to interact with Azure\n",
    "    \"\"\"\n",
    "    BRONZE_ADLS_PATH: str = \"abfs://bronze@[ACCOUNT_NAME].dfs.core.windows.net/\"\n",
    "    SILVER_ADLS_PATH: str = \"abfss://silver@[ACCOUNT_NAME].dfs.core.windows.net\"\n",
    "    GOLD_ADLS_PATH: str = \"abfss://gold@[ACCOUNT_NAME].dfs.core.windows.net\"\n",
    "\n",
    "class Location(NamedTuple):\n",
    "    \"\"\"\n",
    "    Immutable object that contains all of the necessary geographic data\n",
    "    \"\"\"\n",
    "    lat_N: float = 51.3769\n",
    "    lat_S: float = 50.7726\n",
    "    lon_E: float = -113.7319\n",
    "    lon_W: float = -114.3362\n",
    "\n",
    "class LocationHelper(Location):\n",
    "    \"\"\"\n",
    "    Provides additional methods to assist with location specific operations\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def get_region(self) -> str:\n",
    "        \"\"\"\n",
    "        Templates the latitudes and longitudes in a string that is expected by EARTHDATA APIs\n",
    "\n",
    "        :return: region\n",
    "        \"\"\"\n",
    "        return f\"[BBOX]N{self.lat_N} S{self.lat_S} W{self.lon_W} E{self.lon_E}\"\n",
    "    \n",
    "    @property\n",
    "    def get_lat_center(self) -> float:\n",
    "        return (0.5 * (self.lat_S + self.lat_N))\n",
    "    \n",
    "    @property\n",
    "    def get_lon_center(self) -> float:\n",
    "        return (0.5 * (self.lon_W + self.lon_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad951afc-8ba3-49fb-8222-ab5993cf6251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SingletonBronzeETL:\n",
    "    \"\"\"\n",
    "    Handles the extraction of data from NASA's EARTHDATA website and does minimal transformations necessary to upload the raw data\n",
    "    to the Bronze layer of the lake.\n",
    "    \"\"\"\n",
    "    def __new__(cls, location_helper, azure_data_lake, local: bool=False):\n",
    "        # Singleton Pattern\n",
    "        if not hasattr(cls, 'instance'):\n",
    "            cls.instance = super(SingletonBronzeETL, cls).__new__(cls)\n",
    "        return cls.instance\n",
    "\n",
    "    def __init__(self, location_helper: LocationHelper, azure_data_lake: AzureDataLake, local):\n",
    "        \"\"\"\n",
    "        :param location_helper: Coordinates of the city being analyzed.\n",
    "        :param azure_data_lake: Contains Paths to the Bronze, Silver, and Gold layers.\n",
    "        :param local: Flag to determine if the notebook is running in azure cloud or locally. Credential management will change depending on the environment.\n",
    "        \"\"\"\n",
    "        self._LAADS_TOKEN = None\n",
    "        self._ACCOUNT_NAME = None\n",
    "        self._ACCOUNT_KEY = None\n",
    "        self._local = local\n",
    "\n",
    "        # Get the keys and tokens from a .env file\n",
    "        if self._local:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "            self._LAADS_TOKEN = os.environ.get(\"LAADS_TOKEN\")\n",
    "            self._ACCOUNT_NAME = os.environ.get(\"ACCOUNT_NAME\")\n",
    "            self._ACCOUNT_KEY = os.environ.get(\"ACCOUNT_KEY\")\n",
    "\n",
    "            # Ensure credentials are not missing\n",
    "            missing_credentials = [cred for cred, val in [\n",
    "                (\"LAADS_TOKEN\", self._LAADS_TOKEN),\n",
    "                (\"ACCOUNT_NAME\", self._ACCOUNT_NAME),\n",
    "                (\"ACCOUNT_KEY\", self._ACCOUNT_KEY),\n",
    "            ] if not val]\n",
    "            \n",
    "            if missing_credentials:\n",
    "                raise ValueError(f\"Missing required environment variables: {missing_credentials}\")\n",
    "        \n",
    "        else:\n",
    "            # NOTE: LAADS_TOKEN is being taken from a AKV (Azure Back Key Vault) secret\n",
    "            #       For more information, please read https://learn.microsoft.com/en-us/azure/databricks/security/secrets/\n",
    "            self._LAADS_TOKEN = dbutils.secrets.get(scope=\"ensf612_project\", key=\"LAADS-TOKEN\")\n",
    "            self._ACCOUNT_NAME = dbutils.secrets.get(scope=\"ensf612_project\", key=\"ACCOUNT-NAME\")\n",
    "\n",
    "\n",
    "        self._NASA_REQUEST_HEADERS = {\n",
    "            \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "            \"Authorization\": f\"Bearer {self._LAADS_TOKEN}\"\n",
    "        }\n",
    "        self._location_helper = location_helper\n",
    "        self._azure_data_lake = azure_data_lake\n",
    "    \n",
    "    # Helper Methods\n",
    "    def _upload_parquet(self, df: pd.DataFrame, datalake_path: str):\n",
    "        datalake_path = datalake_path.replace(\"[ACCOUNT_NAME]\", self._ACCOUNT_NAME)\n",
    "        if self._local:\n",
    "            df.to_parquet(datalake_path, \n",
    "                        index=False, \n",
    "                        storage_options={\n",
    "                                \"account_name\": self._ACCOUNT_NAME,\n",
    "                                \"account_key\": self._ACCOUNT_KEY\n",
    "                            }\n",
    "                        )\n",
    "        else:\n",
    "            from pyspark.sql import SparkSession, DataFrame\n",
    "            spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").getOrCreate()\n",
    "            df = spark.createDataFrame(df)\n",
    "            df.write.mode(\"overwrite\").format(\"parquet\").save(datalake_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d45dcc2-5451-4314-a522-75e49363062c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MCD06COSP(SingletonBronzeETL):\n",
    "    def __init__(self, location_helper: LocationHelper, azure_data_lake: AzureDataLake, local: bool=False):\n",
    "        super(MCD06COSP, self).__init__(location_helper, azure_data_lake, local)\n",
    "        self._dataset_info = {\n",
    "            \"base_details_url\": \"https://ladsweb.modaps.eosdis.nasa.gov/api/v2/content/details\",\n",
    "            \"base_archive_url\": \"https://ladsweb.modaps.eosdis.nasa.gov/api/v2/content/archives\",\n",
    "            \"product\": \"MCD06COSP_D3_MODIS\"\n",
    "        }\n",
    "\n",
    "    # Helper Methods\n",
    "    def _compute_cloud_fraction_from_bytes(self, file_bytes: bytes, group_name: str='Cloud_Mask_Fraction') -> float:\n",
    "        with Dataset('inmem', mode='r', memory=file_bytes) as nc:\n",
    "            lats = nc.variables['latitude'][:].astype(float)\n",
    "            lons = nc.variables['longitude'][:].astype(float)\n",
    "            nlat = lats.size\n",
    "            nlon = lons.size\n",
    "            \n",
    "            if group_name not in nc.groups:\n",
    "                raise KeyError(f\"Group '{group_name}' not in {list(nc.groups.keys())}\")\n",
    "\n",
    "            grp = nc.groups[group_name]\n",
    "            if 'Mean' not in grp.variables:\n",
    "                raise KeyError(f\"'Mean' not found in group '{group_name}': {list(grp.variables.keys())}\")\n",
    "            \n",
    "            cloud = grp.variables['Mean'][:].astype(float)\n",
    "            shape = cloud.shape\n",
    "            if shape == (nlat, nlon):\n",
    "                lat_first = True\n",
    "            elif shape == (nlon, nlat):\n",
    "                lat_first = False\n",
    "            else:\n",
    "                raise ValueError(f'Unexpected cloud array shape {shape} with nlat={nlat}, nlon={nlon}')\n",
    "            \n",
    "            lat_idx0 = int(np.argmin(np.abs(lats - self._location_helper.get_lat_center)))\n",
    "            lon_idx0 = int(np.argmin(np.abs(lons - self._location_helper.get_lon_center)))\n",
    "            lat_idx = np.arange(max(0, lat_idx0 - 1), min(nlat, lat_idx0 + 2))\n",
    "            lon_idx = np.arange(max(0, lon_idx0 - 1), min(nlon, lon_idx0 + 2))\n",
    "\n",
    "            if lat_idx.size == 0 or lon_idx.size == 0:\n",
    "                return float('nan')\n",
    "            if lat_first:\n",
    "                subset = cloud[np.ix_(lat_idx, lon_idx)]\n",
    "            else:\n",
    "                subset = cloud[np.ix_(lon_idx, lat_idx)]\n",
    "            subset = np.where(subset < -1e5, np.nan, subset)\n",
    "            if np.all(np.isnan(subset)):\n",
    "                return float('nan')\n",
    "            return float(np.nanmean(subset))\n",
    "    \n",
    "    def _extract_path(self, item: dict) -> str:\n",
    "        if \"downloadsLink\" in item:\n",
    "            url = item[\"downloadsLink\"]\n",
    "            return url.split(\"/archives/\", 1)[1]\n",
    "        if \"self\" in item:\n",
    "            url = item[\"self\"]\n",
    "            return url.split(\"/details/\", 1)[1]\n",
    "        if \"name\" in item:\n",
    "            return item[\"name\"]\n",
    "        raise KeyError(f\"No archive/path field in {list(item.keys())}\")\n",
    "    \n",
    "    def _fetch_summer_details_for_year(self, year: int) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        d = date(year, 6, 1)\n",
    "        end = date(year, 8, 31)\n",
    "\n",
    "        while d <= end:\n",
    "            temporal_range = f\"{d:%Y-%m-%d}..{d:%Y-%m-%d}\"\n",
    "            params = {\n",
    "                \"products\": self._dataset_info[\"product\"],\n",
    "                \"temporalRanges\": temporal_range,\n",
    "                \"regions\": self._location_helper.get_region,\n",
    "                \"formats\": \"json\",\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(\n",
    "                    self._dataset_info[\"base_details_url\"],\n",
    "                    params=params,\n",
    "                    headers=self._NASA_REQUEST_HEADERS,\n",
    "                    timeout=60,\n",
    "                )\n",
    "                \n",
    "                if resp.status_code >= 500:\n",
    "                    print(f\"Year {year}, date {d}: LAADS {resp.status_code}, skipping\")\n",
    "                    d += timedelta(days=1)\n",
    "                    continue\n",
    "\n",
    "                resp.raise_for_status()\n",
    "                data = resp.json()\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Year {year}, date {d}: request failed ({e}), skipping\")\n",
    "                d += timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            items = []\n",
    "            if isinstance(data, dict) and \"content\" in data:\n",
    "                items = data[\"content\"]\n",
    "\n",
    "            elif isinstance(data, list):\n",
    "                items = data\n",
    "\n",
    "            for it in items:\n",
    "                rel_path = self._extract_path(it)\n",
    "                fname = it.get(\"name\", rel_path.split(\"/\")[-1])\n",
    "\n",
    "                if \"dataDay\" in it:\n",
    "                    left = it[\"dataDay\"].split(\"=\", 1)[0].strip()\n",
    "                    year_str, doy_str = left.split(\"-\")\n",
    "                    dd = datetime.strptime(year_str + doy_str.zfill(3), \"%Y%j\").date()\n",
    "                else:\n",
    "                    dd = self.date_from_mcd06cosp_filename(fname).date()\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"year\": dd.year,\n",
    "                        \"date\": dd,\n",
    "                        \"rel_path\": rel_path,\n",
    "                        \"file_name\": fname,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            d += timedelta(days=1)\n",
    "\n",
    "        if not rows:\n",
    "            return pd.DataFrame(columns=[\"year\", \"date\", \"rel_path\", \"file_name\"])\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def _upload_year(self, year: int):\n",
    "        df_raw_year = self._fetch_summer_details_for_year(year)\n",
    "        if df_raw_year.empty:\n",
    "            print(f\"Year {year}: no data\")\n",
    "            return None\n",
    "        \n",
    "        bronze_path_year = str(Path(self._azure_data_lake.BRONZE_ADLS_PATH).joinpath(f\"laads_links_summer_{year}.parquet\"))\n",
    "        self._upload_parquet(df_raw_year, bronze_path_year)\n",
    "        print(f\"Wrote RAW {bronze_path_year} with {len(df_raw_year)} rows\")\n",
    "        return bronze_path_year\n",
    "            \n",
    "    # Public Methods\n",
    "    def create_bronze_layer(self, years, max_worker_threads=12):\n",
    "        paths = []\n",
    "        # Get all of the RAW data within a given range of years\n",
    "        with ThreadPoolExecutor(max_workers=max_worker_threads) as ex:\n",
    "            futures = {ex.submit(self._upload_year, y): y for y in years}\n",
    "            for fut in as_completed(futures):\n",
    "                p = fut.result()\n",
    "                if p is not None:\n",
    "                    paths.append(p)\n",
    "        \n",
    "        # Upload a manifest file of all the years\n",
    "        paths_df = pd.DataFrame({'raw_path': paths})\n",
    "        manifest_path = str(Path(self._azure_data_lake.BRONZE_ADLS_PATH).joinpath(\"laads_links_manifest.parquet\"))\n",
    "        self._upload_parquet(paths_df, manifest_path)\n",
    "        return manifest_path\n",
    "        \n",
    "    def date_from_mcd06cosp_filename(self, fname: str) -> datetime:\n",
    "        m = re.search(r'\\.A(\\d{4})(\\d{3})', fname)\n",
    "        if not m:\n",
    "            raise ValueError(f'Could not parse date from {fname}')\n",
    "        year = int(m.group(1))\n",
    "        doy = int(m.group(2))\n",
    "        return datetime.strptime(f'{year}{doy:03d}', '%Y%j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1db44595-b952-421e-a0ad-0087c3507558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set the Environment\n",
    "**NOTE:** Only run the cell pertaining to your environment. The TA's grading this project does not have access to our Azure Cloud, so they must run the notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa811a6-27f3-4b50-8ae4-0acb5fddab23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Cloud Setup\n",
    "bronze_etl = MCD06COSP(location_helper=LocationHelper(), azure_data_lake=AzureDataLake())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4c48c8c-66cc-453b-9df4-4019b239fe3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Local Setup\n",
    "bronze_etl = MCD06COSP(location_helper=LocationHelper(), azure_data_lake=AzureDataLake(), local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b228290a-1bd6-4985-981a-e304c92e5850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ea8dc5-0d7c-4335-80ba-b3ae7153de37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2001: no data\nYear 2000: no data\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5542673593405681>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m years \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2000\u001B[39m, \u001B[38;5;241m2026\u001B[39m))\n",
       "\u001B[0;32m----> 2\u001B[0m manifest_path \u001B[38;5;241m=\u001B[39m bronze_etl\u001B[38;5;241m.\u001B[39mcreate_bronze_layer(years)\n",
       "\n",
       "File \u001B[0;32m<command-5542673593405680>, line 151\u001B[0m, in \u001B[0;36mMCD06COSP.create_bronze_layer\u001B[0;34m(self, years, max_worker_threads)\u001B[0m\n",
       "\u001B[1;32m    149\u001B[0m futures \u001B[38;5;241m=\u001B[39m {ex\u001B[38;5;241m.\u001B[39msubmit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upload_year, y): y \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m years}\n",
       "\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fut \u001B[38;5;129;01min\u001B[39;00m as_completed(futures):\n",
       "\u001B[0;32m--> 151\u001B[0m     p \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mresult()\n",
       "\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    153\u001B[0m         paths\u001B[38;5;241m.\u001B[39mappend(p)\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
       "\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n",
       "\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n",
       "\u001B[1;32m    451\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n",
       "\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n",
       "\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n",
       "\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n",
       "\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
       "\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n",
       "\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
       "\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
       "\n",
       "File \u001B[0;32m<command-5542673593405680>, line 140\u001B[0m, in \u001B[0;36mMCD06COSP._upload_year\u001B[0;34m(self, year)\u001B[0m\n",
       "\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    139\u001B[0m bronze_path_year \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(Path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_azure_data_lake\u001B[38;5;241m.\u001B[39mBRONZE_ADLS_PATH)\u001B[38;5;241m.\u001B[39mjoinpath(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlaads_links_summer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00myear\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[0;32m--> 140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upload_parquet(df_raw_year, bronze_path_year)\n",
       "\u001B[1;32m    141\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrote RAW \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze_path_year\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df_raw_year)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bronze_path_year\n",
       "\n",
       "File \u001B[0;32m<command-5542673593405673>, line 70\u001B[0m, in \u001B[0;36mSingletonBronzeETL._upload_parquet\u001B[0;34m(self, df, datalake_path)\u001B[0m\n",
       "\u001B[1;32m     68\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython Spark SQL basic example\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
       "\u001B[1;32m     69\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(df)\n",
       "\u001B[0;32m---> 70\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(datalake_path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/readwriter.py:703\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    701\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n",
       "\u001B[0;32m--> 703\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    705\u001B[0m )\n",
       "\u001B[1;32m    706\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1589\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1587\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1588\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_command_in_plan(req\u001B[38;5;241m.\u001B[39mplan, command)\n",
       "\u001B[0;32m-> 1589\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1590\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1591\u001B[0m )\n",
       "\u001B[1;32m   1592\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1593\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2140\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2137\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2141\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2142\u001B[0m     ):\n",
       "\u001B[1;32m   2143\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2144\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2116\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2115\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2436\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2434\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2435\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2436\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2437\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2438\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2514\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2510\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2512\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2514\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2515\u001B[0m                 info,\n",
       "\u001B[1;32m   2516\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2517\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2518\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2519\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2520\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2522\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2523\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2524\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2525\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2526\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2527\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnknownException\u001B[0m: (shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException) abfs:/bronze@[REDACTED].dfs.core.windows.net/laads_links_summer_2011.parquet has invalid authority.\n",
       "\n",
       "JVM stacktrace:\n",
       "shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getAuthorityParts(AzureBlobFileSystemStore.java:481)\n",
       "\tat com.databricks.common.filesystem.LokiABFS$.computeKey(LokiABFS.scala:127)\n",
       "\tat com.databricks.sql.io.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:104)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.initialize(LokiFileSystem.scala:181)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:459)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:440)\n",
       "\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:579)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:340)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:185)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:152)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4042)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3425)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnknownException",
        "evalue": "(shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException) abfs:/bronze@ensf612project.dfs.core.windows.net/laads_links_summer_2011.parquet has invalid authority.\n\nJVM stacktrace:\nshaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getAuthorityParts(AzureBlobFileSystemStore.java:481)\n\tat com.databricks.common.filesystem.LokiABFS$.computeKey(LokiABFS.scala:127)\n\tat com.databricks.sql.io.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:104)\n\tat com.databricks.sql.io.LokiFileSystem.initialize(LokiFileSystem.scala:181)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:459)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:440)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:579)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:340)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:185)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:152)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4042)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3425)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>UnknownException</span>: (shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException) abfs:/bronze@[REDACTED].dfs.core.windows.net/laads_links_summer_2011.parquet has invalid authority.\n\nJVM stacktrace:\nshaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getAuthorityParts(AzureBlobFileSystemStore.java:481)\n\tat com.databricks.common.filesystem.LokiABFS$.computeKey(LokiABFS.scala:127)\n\tat com.databricks.sql.io.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:104)\n\tat com.databricks.sql.io.LokiFileSystem.initialize(LokiFileSystem.scala:181)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:459)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:440)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:579)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:340)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:185)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:152)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4042)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3425)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XXKCM",
        "stackTrace": "shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getAuthorityParts(AzureBlobFileSystemStore.java:481)\n\tat com.databricks.common.filesystem.LokiABFS$.computeKey(LokiABFS.scala:127)\n\tat com.databricks.sql.io.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:104)\n\tat com.databricks.sql.io.LokiFileSystem.initialize(LokiFileSystem.scala:181)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:459)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:440)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:579)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:340)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:185)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:152)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4042)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3425)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-5542673593405681>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m years \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2000\u001B[39m, \u001B[38;5;241m2026\u001B[39m))\n\u001B[0;32m----> 2\u001B[0m manifest_path \u001B[38;5;241m=\u001B[39m bronze_etl\u001B[38;5;241m.\u001B[39mcreate_bronze_layer(years)\n",
        "File \u001B[0;32m<command-5542673593405680>, line 151\u001B[0m, in \u001B[0;36mMCD06COSP.create_bronze_layer\u001B[0;34m(self, years, max_worker_threads)\u001B[0m\n\u001B[1;32m    149\u001B[0m futures \u001B[38;5;241m=\u001B[39m {ex\u001B[38;5;241m.\u001B[39msubmit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upload_year, y): y \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m years}\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fut \u001B[38;5;129;01min\u001B[39;00m as_completed(futures):\n\u001B[0;32m--> 151\u001B[0m     p \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mresult()\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    153\u001B[0m         paths\u001B[38;5;241m.\u001B[39mappend(p)\n",
        "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
        "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
        "File \u001B[0;32m<command-5542673593405680>, line 140\u001B[0m, in \u001B[0;36mMCD06COSP._upload_year\u001B[0;34m(self, year)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    139\u001B[0m bronze_path_year \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(Path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_azure_data_lake\u001B[38;5;241m.\u001B[39mBRONZE_ADLS_PATH)\u001B[38;5;241m.\u001B[39mjoinpath(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlaads_links_summer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00myear\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upload_parquet(df_raw_year, bronze_path_year)\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrote RAW \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze_path_year\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df_raw_year)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bronze_path_year\n",
        "File \u001B[0;32m<command-5542673593405673>, line 70\u001B[0m, in \u001B[0;36mSingletonBronzeETL._upload_parquet\u001B[0;34m(self, df, datalake_path)\u001B[0m\n\u001B[1;32m     68\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython Spark SQL basic example\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m     69\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(df)\n\u001B[0;32m---> 70\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(datalake_path)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/readwriter.py:703\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    701\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m--> 703\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    705\u001B[0m )\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1589\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1587\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1588\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_command_in_plan(req\u001B[38;5;241m.\u001B[39mplan, command)\n\u001B[0;32m-> 1589\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1590\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1591\u001B[0m )\n\u001B[1;32m   1592\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1593\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2140\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2137\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2141\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2142\u001B[0m     ):\n\u001B[1;32m   2143\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2144\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2116\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2115\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2436\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2434\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2435\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2436\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2437\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2438\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2514\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2510\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2512\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2514\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2515\u001B[0m                 info,\n\u001B[1;32m   2516\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2517\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2518\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2519\u001B[0m                 status_code,\n\u001B[1;32m   2520\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2522\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2523\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2524\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2525\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2526\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2527\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mUnknownException\u001B[0m: (shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException) abfs:/bronze@[REDACTED].dfs.core.windows.net/laads_links_summer_2011.parquet has invalid authority.\n\nJVM stacktrace:\nshaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidUriAuthorityException\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getAuthorityParts(AzureBlobFileSystemStore.java:481)\n\tat com.databricks.common.filesystem.LokiABFS$.computeKey(LokiABFS.scala:127)\n\tat com.databricks.sql.io.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:104)\n\tat com.databricks.sql.io.LokiFileSystem.initialize(LokiFileSystem.scala:181)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:459)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:440)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:579)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:340)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:185)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:152)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4042)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3425)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "years = list(range(2000, 2026))\n",
    "manifest_path = bronze_etl.create_bronze_layer(years)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}