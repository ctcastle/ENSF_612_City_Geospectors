{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "abf47bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\toshi\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install --quiet xarray netCDF4 h5netcdf requests pandas numpy pyarrow adlfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e819d603-b5ef-4a01-8a49-e4bdb46fcf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DefaultEndpointsProtocol=https;AccountName=ucalgarydatalake01;AccountKey=vKNY9SpzF1ydaZ6X8it3s2OOkXxS2v9qUcMLlNBUCUnVzJOunsDz0JronE+G4MZUif1tMAaMGfTt+AStUmavaA==;EndpointSuffix=core.windows.net\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get('LINKS_FS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c0b79cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_path: abfss://raw@ucalgarydatalake01.dfs.core.windows.net/laads_links.parquet len: 71\n",
      "curated_path: abfss://curated@ucalgarydatalake01.dfs.core.windows.net/calgary_cloud_fraction.parquet len: 86\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Reads the .env file in the current directory\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xarray as xr\n",
    "\n",
    "# ==== Load required environment variables ====\n",
    "import os\n",
    "\n",
    "LAADS_TOKEN  = os.environ.get(\"LAADS_TOKEN\")\n",
    "ACCOUNT_NAME = os.environ.get(\"ACCOUNT_NAME\")\n",
    "ACCOUNT_KEY  = os.environ.get(\"ACCOUNT_KEY\")\n",
    "\n",
    "missing = [name for name, val in [\n",
    "    (\"LAADS_TOKEN\", LAADS_TOKEN),\n",
    "    (\"ACCOUNT_NAME\", ACCOUNT_NAME),\n",
    "    (\"ACCOUNT_KEY\", ACCOUNT_KEY),\n",
    "] if not val]\n",
    "\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required environment variables: {missing}\")\n",
    "\n",
    "storage_options = {\n",
    "    \"account_name\": ACCOUNT_NAME,\n",
    "    \"account_key\": ACCOUNT_KEY,\n",
    "}\n",
    "\n",
    "# Containers (filesystems) are literally named raw / curated / gold\n",
    "RAW_CONTAINER     = \"raw\"\n",
    "CURATED_CONTAINER = \"curated\"\n",
    "GOLD_CONTAINER    = \"gold\"  # not used yet\n",
    "\n",
    "# Keep blob names *very* short\n",
    "raw_path = f\"abfss://{RAW_CONTAINER}@{ACCOUNT_NAME}.dfs.core.windows.net/laads_links.parquet\"\n",
    "curated_path = f\"abfss://{CURATED_CONTAINER}@{ACCOUNT_NAME}.dfs.core.windows.net/calgary_cloud_fraction.parquet\"\n",
    "\n",
    "print(\"raw_path:\", raw_path, \"len:\", len(raw_path))\n",
    "print(\"curated_path:\", curated_path, \"len:\", len(curated_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8275a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules returned: 1\n",
      "Example keys: ['archiveSets', 'cksum', 'collections', 'dataDay', 'downloadsLink', 'fileId', 'md5sum', 'mtime', 'name', 'products', 'resourceType', 'self', 'size', 'start', 'status']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel_path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>raw_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc</td>\n",
       "      <td>MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc</td>\n",
       "      <td>{'archiveSets': 62, 'cksum': '771683725', 'col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           rel_path  \\\n",
       "0  MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc   \n",
       "\n",
       "                                          file_name  \\\n",
       "0  MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc   \n",
       "\n",
       "                                            raw_json  \n",
       "0  {'archiveSets': 62, 'cksum': '771683725', 'col...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Query LAADS `content/details` ---\n",
    "params_details = {\n",
    "    'products': PRODUCT,\n",
    "    'temporalRanges': TEMPORAL_RANGE,\n",
    "    'regions': REGION_PARAM,\n",
    "    'formats': 'json',\n",
    "}\n",
    "\n",
    "resp = requests.get(BASE_DETAILS_URL, params=params_details, headers=headers_nasa)\n",
    "resp.raise_for_status()\n",
    "details = resp.json()\n",
    "\n",
    "# Extract list of items\n",
    "if isinstance(details, dict) and 'content' in details:\n",
    "    items = details['content']\n",
    "elif isinstance(details, list):\n",
    "    items = details\n",
    "else:\n",
    "    raise ValueError('Unexpected JSON structure from LAADS; inspect `details`.')\n",
    "\n",
    "print('Granules returned:', len(items))\n",
    "if items:\n",
    "    print('Example keys:', list(items[0].keys()))\n",
    "\n",
    "PATH_FIELD_CANDIDATES = ['archivePath', 'path', 'name', 'fileName']\n",
    "\n",
    "def extract_path(item):\n",
    "    for k in PATH_FIELD_CANDIDATES:\n",
    "        if k in item:\n",
    "            return item[k]\n",
    "    raise KeyError(f'No archive/path field found in keys: {list(item.keys())}')\n",
    "\n",
    "rows = []\n",
    "for it in items:\n",
    "    rel_path = extract_path(it)\n",
    "    fname = rel_path.split('/')[-1]\n",
    "    rows.append({\n",
    "        'rel_path': rel_path,\n",
    "        'file_name': fname,\n",
    "        'raw_json': it,\n",
    "    })\n",
    "\n",
    "df_raw = pd.DataFrame(rows)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36c7e8-b14d-4d5f-bab5-bbff3e22aca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff5c4de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw metadata written to: abfss://raw@ucalgarydatalake01.dfs.core.windows.net/laads_links.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- Write raw metadata into ADLS (`raw` layer) ---\n",
    "df_raw.to_parquet(\n",
    "    raw_path,\n",
    "    index=False,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "print('Raw metadata written to:', raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53ad01d5-756b-4272-9556-49197a377a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel_path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>raw_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc</td>\n",
       "      <td>MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc</td>\n",
       "      <td>{'archiveSets': 62, 'cksum': '771683725', 'col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           rel_path  \\\n",
       "0  MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc   \n",
       "\n",
       "                                          file_name  \\\n",
       "0  MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc   \n",
       "\n",
       "                                            raw_json  \n",
       "0  {'archiveSets': 62, 'cksum': '771683725', 'col...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load raw links from the lake ---\n",
    "df_raw = pd.read_parquet(raw_path, storage_options=storage_options)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa68ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_mcd06cosp_filename(filename: str) -> datetime:\n",
    "    \"\"\"Extract date from MCD06COSP_D3_MODIS filename (AYYYYDDD).\"\"\"\n",
    "    m = re.search(r\"\\.A(\\d{4})(\\d{3})\", filename)\n",
    "    if not m:\n",
    "        raise ValueError(f'Could not parse date from {filename}')\n",
    "    year = int(m.group(1))\n",
    "    doy  = int(m.group(2))\n",
    "    return datetime.strptime(f\"{year}{doy:03d}\", \"%Y%j\")\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def compute_cloud_fraction_from_bytes(\n",
    "    file_bytes: bytes,\n",
    "    lat_s: float = LAT_S,\n",
    "    lat_n: float = LAT_N,\n",
    "    lon_w: float = LON_W,\n",
    "    lon_e: float = LON_E,\n",
    "    group_name: str = \"Cloud_Mask_Fraction\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Open one MCD06COSP_D3_MODIS netCDF file from raw bytes and compute\n",
    "    the mean cloud fraction near your Calgary bbox.\n",
    "\n",
    "    Strategy:\n",
    "      - take the center of the bbox\n",
    "      - find the *nearest* latitude/longitude grid indices\n",
    "      - average a small 3x3 window around that grid cell\n",
    "    \"\"\"\n",
    "    with Dataset(\"inmem\", mode=\"r\", memory=file_bytes) as nc:\n",
    "        # 1) Lat/lon arrays (1D)\n",
    "        lats = nc.variables[\"latitude\"][:].astype(float)   # (nlat,)\n",
    "        lons = nc.variables[\"longitude\"][:].astype(float)  # (nlon,)\n",
    "\n",
    "        nlat = lats.size\n",
    "        nlon = lons.size\n",
    "\n",
    "        if group_name not in nc.groups:\n",
    "            raise KeyError(f\"Group '{group_name}' not in {list(nc.groups.keys())}\")\n",
    "\n",
    "        grp = nc.groups[group_name]\n",
    "        if \"Mean\" not in grp.variables:\n",
    "            raise KeyError(\n",
    "                f\"'Mean' not found in group '{group_name}': \"\n",
    "                f\"{list(grp.variables.keys())}\"\n",
    "            )\n",
    "\n",
    "        cloud = grp.variables[\"Mean\"][:].astype(float)\n",
    "        shape = cloud.shape\n",
    "\n",
    "        if shape == (nlat, nlon):\n",
    "            lat_first = True       # cloud[lat, lon]\n",
    "        elif shape == (nlon, nlat):\n",
    "            lat_first = False      # cloud[lon, lat]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected cloud array shape {shape} with nlat={nlat}, nlon={nlon}\"\n",
    "            )\n",
    "\n",
    "        lat_center = 0.5 * (lat_s + lat_n)\n",
    "        lon_center = 0.5 * (lon_w + lon_e)\n",
    "\n",
    "        lat_idx0 = int(np.argmin(np.abs(lats - lat_center)))\n",
    "        lon_idx0 = int(np.argmin(np.abs(lons - lon_center)))\n",
    "\n",
    "        lat_idx = np.arange(max(0, lat_idx0 - 1), min(nlat, lat_idx0 + 2))\n",
    "        lon_idx = np.arange(max(0, lon_idx0 - 1), min(nlon, lon_idx0 + 2))\n",
    "\n",
    "        if lat_idx.size == 0 or lon_idx.size == 0:\n",
    "            return float(\"nan\")\n",
    "\n",
    "        if lat_first:\n",
    "            subset = cloud[np.ix_(lat_idx, lon_idx)]\n",
    "        else:\n",
    "            subset = cloud[np.ix_(lon_idx, lat_idx)]\n",
    "\n",
    "        subset = np.where(subset < -1e5, np.nan, subset)  # treat big negatives as fill\n",
    "\n",
    "        if np.all(np.isnan(subset)):\n",
    "            return float(\"nan\")\n",
    "\n",
    "        return float(np.nanmean(subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139977cc-b7bc-4b03-bc39-8f1c4db7aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b4e6afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fetch] MCD06COSP_D3_MODIS.A2010001.062.2022124195350.nc (2010-01-01)\n",
      "   -> mean cloud fraction over bbox: 0.856668038528928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cloud_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0.856668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  cloud_fraction\n",
       "0 2010-01-01        0.856668"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "for _, row in df_raw.iterrows():\n",
    "    rel_path = row['rel_path']\n",
    "    fname = row['file_name']\n",
    "    dt = date_from_mcd06cosp_filename(fname)\n",
    "    url = f\"{BASE_ARCHIVES_URL}/{rel_path.lstrip('/')}\"\n",
    "    print(f\"[fetch] {fname} ({dt.date()})\")\n",
    "    r = requests.get(url, headers=headers_nasa, stream=True)\n",
    "    r.raise_for_status()\n",
    "    cf = compute_cloud_fraction_from_bytes(r.content)\n",
    "    print(f\"   -> mean cloud fraction over bbox: {cf}\")\n",
    "    records.append({'date': dt, 'cloud_fraction': cf})\n",
    "\n",
    "df_curated = pd.DataFrame(records)\n",
    "df_curated['date'] = pd.to_datetime(df_curated['date'])\n",
    "df_curated = df_curated.sort_values('date').reset_index(drop=True)\n",
    "df_curated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1453f29-f262-4dbd-b10a-cf7a2877bd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT VARIABLES:\n",
      "['latitude', 'longitude']\n",
      "\n",
      "GROUPS:\n",
      "['Solar_Zenith', 'Solar_Azimuth', 'Sensor_Zenith', 'Sensor_Azimuth', 'Cloud_Top_Pressure', 'Cloud_Mask_Fraction', 'Cloud_Mask_Fraction_Low', 'Cloud_Mask_Fraction_Mid', 'Cloud_Mask_Fraction_High', 'Cloud_Optical_Thickness_Liquid', 'Cloud_Optical_Thickness_Ice', 'Cloud_Optical_Thickness_Total', 'Cloud_Optical_Thickness_PCL_Liquid', 'Cloud_Optical_Thickness_PCL_Ice', 'Cloud_Optical_Thickness_PCL_Total', 'Cloud_Optical_Thickness_Log10_Liquid', 'Cloud_Optical_Thickness_Log10_Ice', 'Cloud_Optical_Thickness_Log10_Total', 'Cloud_Particle_Size_Liquid', 'Cloud_Particle_Size_Ice', 'Cloud_Particle_Size_PCL_Liquid', 'Cloud_Particle_Size_PCL_Ice', 'Cloud_Water_Path_Liquid', 'Cloud_Water_Path_Ice', 'Cloud_Water_Path_PCL_Liquid', 'Cloud_Water_Path_PCL_Ice', 'Cloud_Retrieval_Fraction_Liquid', 'Cloud_Retrieval_Fraction_Ice', 'Cloud_Retrieval_Fraction_Total', 'Cloud_Retrieval_Fraction_PCL_Liquid', 'Cloud_Retrieval_Fraction_PCL_Ice', 'Cloud_Retrieval_Fraction_PCL_Total']\n",
      "\n",
      "Latitude variable 'latitude': shape=(180,), min=-89.5, max=89.5\n",
      "\n",
      "Longitude variable 'longitude': shape=(360,), min=-179.5, max=179.5\n",
      "\n",
      "Cloud_Mask_Fraction variables: ['Mean', 'Standard_Deviation', 'Sum', 'Pixel_Counts', 'Sum_Squares']\n",
      "Cloud fraction shape: (360, 180)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_bytes = r.content\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "with Dataset(\"inmem\", mode=\"r\", memory=test_bytes) as nc:\n",
    "    print(\"ROOT VARIABLES:\")\n",
    "    print(list(nc.variables.keys()))\n",
    "\n",
    "    print(\"\\nGROUPS:\")\n",
    "    print(list(nc.groups.keys()))\n",
    "\n",
    "    # print lat/lon shapes + min/max\n",
    "    for cand in [\"latitude\", \"lat\", \"Latitude\", \"LAT\"]:\n",
    "        if cand in nc.variables:\n",
    "            lat = nc.variables[cand][:]\n",
    "            print(f\"\\nLatitude variable '{cand}': shape={lat.shape}, min={lat.min()}, max={lat.max()}\")\n",
    "            break\n",
    "\n",
    "    for cand in [\"longitude\", \"lon\", \"Longitude\", \"LON\"]:\n",
    "        if cand in nc.variables:\n",
    "            lon = nc.variables[cand][:]\n",
    "            print(f\"\\nLongitude variable '{cand}': shape={lon.shape}, min={lon.min()}, max={lon.max()}\")\n",
    "            break\n",
    "\n",
    "    if \"Cloud_Mask_Fraction\" in nc.groups:\n",
    "        grp = nc.groups[\"Cloud_Mask_Fraction\"]\n",
    "        print(\"\\nCloud_Mask_Fraction variables:\", list(grp.variables.keys()))\n",
    "        data = grp.variables[\"Mean\"][:]\n",
    "        print(\"Cloud fraction shape:\", data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df3ca7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated dataset written to: abfss://curated@ucalgarydatalake01.dfs.core.windows.net/calgary_cloud_fraction.parquet\n"
     ]
    }
   ],
   "source": [
    "df_curated.to_parquet(\n",
    "    curated_path,\n",
    "    index=False,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "print('Curated dataset written to:', curated_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6028b49-3d0c-40e9-bc69-d9b08b9c674d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958b29a-ac49-4bfd-8405-38e937970b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757384e5-846d-4e55-8f66-3e2718a5b758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
